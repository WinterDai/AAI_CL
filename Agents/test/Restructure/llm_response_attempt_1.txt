```xml
<thoughts>
1. **检查目标**: 验证 netlist/SPEF 文件的版本元数据是否正确
   - 从 STA log 中提取 netlist/SPEF 文件路径
   - 解析 netlist 文件头部注释获取工具版本和日期
   - 解析 SPEF 文件的 IEEE 1481 标准头部字段 (*VERSION, *DATE, *PROGRAM 等)
   - 处理 SPEF 跳过场景 (synthesis 阶段)

2. **数据流**: STA_Log → 提取文件路径 → 解析 Netlist/SPEF → 提取版本元数据 → 验证格式/值
   - STA log: 包含 read_netlist 命令、SPEF 跳过信息、flow_step 等
   - Netlist: Verilog 文件头部注释 (// Generated by, // Date)
   - SPEF: IEEE 1481 标准头部 (*VERSION, *DATE, *PROGRAM, *VENDOR)

3. **辅助方法** (复用 Golden Pattern):
   - `_parse_sta_log()`: 解析 STA log 提取文件路径、状态、元数据
   - `_parse_netlist_file()`: 解析 netlist 文件头部提取版本信息
   - `_parse_spef_file()`: 解析 SPEF 文件头部提取版本元数据
   - `_build_name_extractor()`: 格式化输出名称 (显示版本和日期)

4. **Pass/Fail 逻辑**:
   - Type1: Netlist 或 SPEF 任一存在版本元数据 → PASS
   - Type2: 所有 pattern_items 匹配 → PASS
   - Type3: 所有 pattern 匹配或已 waive → PASS
   - Type4: 版本元数据存在或 flow stage 匹配 waiver → PASS

5. **边缘情况处理** (Critical Business Logic):
   - **文件路径存在但不可访问**: status='Success' 由 success indicator 决定，不是文件存在性
   - **SPEF 跳过**: Type1/4 计入 missing_items (影响 is_pass)，Type2/3 计入 extra_items (不影响 is_pass)
   - **解析错误**: 计入 extra_items (告警但不影响 is_pass)

6. **Metadata 追踪**:
   - 使用 `self._metadata` 字典存储每个发现项的行号和文件路径
   - 格式: `{'netlist_success': {'line_number': N, 'file_path': str}}`
   - 在构建 DetailItem 时从 metadata 提取 line_number 和 file_path
</thoughts>

<class_name>
NetlistSpefVersionChecker
</class_name>

<class_constants>
# Found/Missing descriptions
FOUND_DESC = "netlist/SPEF version metadata found"
MISSING_DESC = "netlist/SPEF version metadata not found or pattern not satisfied"
WAIVED_DESC = "version check waived for current flow stage"
EXTRA_DESC = "Additional information or warnings"

# Detailed reasons
FOUND_REASON = "Version metadata extracted from {file_type}: {version_string}"
MISSING_REASON = "Version metadata missing in {file_type} or pattern '{pattern}' not matched"
WAIVED_BASE_REASON = "Version check waived: {waiver_name}"
EXTRA_REASON = "Flow stage: {flow_step}, SPEF reading status: {spef_status}"
UNUSED_WAIVER_REASON = "Waiver '{waiver_name}' defined but not applied (version metadata found)"
</class_constants>

<name_extractor_method>
def _build_name_extractor(self):
    """返回 name_extractor 函数，用于格式化输出"""
    def extract_name(name: str, metadata: Any) -> str:
        if isinstance(metadata, dict):
            version = metadata.get('version', '')
            date = metadata.get('date', '')
            path = metadata.get('path', '')
            
            # Format: "Name: path, Version: X, Date: Y"
            parts = [name]
            if path:
                parts.append(f"Path: {Path(path).name}")
            if version:
                parts.append(f"Version: {version}")
            if date:
                parts.append(f"Date: {date}")
            
            return ", ".join(parts)
        return name
    return extract_name
</name_extractor_method>

<parse_method>
def _parse_input_files(self) -> Dict[str, Any]:
    """
    Parse input files to extract netlist/SPEF version metadata.
    
    Returns:
        Dict[str, Any] with structure:
        {
            'netlist_info': {
                'status': 'Success'/'Not Found',
                'path': str (if file exists),
                'relative_path': str (if file doesn't exist but path found),
                'version': str,
                'date': str,
                'top_module': str
            },
            'spef_info': {
                'status': 'Success'/'Skipped'/'Not Found',
                'path': str,
                'version': str,
                'date': str,
                'program': str,
                'vendor': str,
                'skip_reason': str (if skipped)
            },
            'flow_step': str,
            'errors': List[str]
        }
    """
    # Initialize metadata tracking
    self._metadata = {}
    
    # Validate input files
    valid_files, missing_files = self.validate_input_files()
    if not valid_files:
        raise ConfigurationError("No valid input files found")
    
    # Initialize result
    result = {
        'netlist_info': {'status': 'Not Found'},
        'spef_info': {'status': 'Not Found'},
        'flow_step': 'Unknown',
        'errors': []
    }
    
    # Parse STA log (primary source)
    sta_log = None
    for file_path in valid_files:
        if 'sta' in file_path.name.lower() and file_path.suffix == '.log':
            sta_log = file_path
            break
    
    if sta_log:
        try:
            sta_data = self._parse_sta_log(sta_log)
            result.update(sta_data)
        except Exception as e:
            result['errors'].append(f"Error parsing STA log: {str(e)}")
    
    # Parse netlist file if path found
    netlist_info = result.get('netlist_info', {})
    if netlist_info.get('path'):
        try:
            netlist_data = self._parse_netlist_file(Path(netlist_info['path']))
            result['netlist_info'].update(netlist_data)
        except Exception as e:
            result['errors'].append(f"Error parsing netlist file: {str(e)}")
    
    # Parse SPEF file if path found and not skipped
    spef_info = result.get('spef_info', {})
    if spef_info.get('status') != 'Skipped' and spef_info.get('path'):
        try:
            spef_data = self._parse_spef_file(Path(spef_info['path']))
            result['spef_info'].update(spef_data)
        except Exception as e:
            result['errors'].append(f"Error parsing SPEF file: {str(e)}")
    
    return result
</parse_method>

<execute_type1>
def _execute_type1(self, parsed_data: Dict[str, Any]) -> CheckResult:
    """Type 1: Boolean check - 存在即 PASS"""
    def parse_data():
        """调用共享的Boolean Check Logic"""
        return self._boolean_check_logic(parsed_data)
    
    return self.execute_boolean_check(
        parse_data_func=parse_data,
        has_waiver=False,
        found_desc=self.FOUND_DESC,
        missing_desc=self.MISSING_DESC,
        extra_desc=self.EXTRA_DESC,
        name_extractor=self._build_name_extractor()
    )
</execute_type1>

<execute_type2>
def _execute_type2(self, parsed_data: Dict[str, Any]) -> CheckResult:
    """Type 2: Value check - 匹配 pattern_items"""
    def parse_data():
        """调用共享的Pattern Check Logic"""
        return self._pattern_check_logic(parsed_data)
    
    return self.execute_value_check(
        parse_data_func=parse_data,
        has_waiver=False,
        found_desc="Pattern matched in version metadata",
        missing_desc="Pattern not found in version metadata",
        extra_desc=self.EXTRA_DESC,
        name_extractor=self._build_name_extractor()
    )
</execute_type2>

<execute_type3>
def _execute_type3(self, parsed_data: Dict[str, Any]) -> CheckResult:
    """Type 3: Value check with waiver - 复用 Type2 逻辑"""
    # Prepare info_items for display
    info_items = {}
    flow_step = parsed_data.get('flow_step', 'Unknown')
    if flow_step != 'Unknown':
        info_items['Flow Step'] = {
            'line_number': 0,
            'file_path': '',
            'reason': f"Current flow step: {flow_step}"
        }
    
    def parse_data():
        """调用共享的Pattern Check Logic (与Type2相同)"""
        return self._pattern_check_logic(parsed_data)
    
    return self.execute_value_check(
        parse_data_func=parse_data,
        has_waiver=True,
        info_items=info_items,
        found_desc="Pattern matched in version metadata",
        missing_desc="Pattern not found in version metadata",
        extra_desc=self.EXTRA_DESC,
        extra_severity=Severity.FAIL,
        name_extractor=self._build_name_extractor()
    )
</execute_type3>

<execute_type4>
def _execute_type4(self, parsed_data: Dict[str, Any]) -> CheckResult:
    """Type 4: Boolean check with waiver - 复用 Type1 逻辑"""
    def parse_data():
        """调用共享的Boolean Check Logic (与Type1相同)"""
        return self._boolean_check_logic(parsed_data)
    
    return self.execute_boolean_check(
        parse_data_func=parse_data,
        has_waiver=True,
        found_desc=self.FOUND_DESC,
        missing_desc=self.MISSING_DESC,
        extra_desc=self.EXTRA_DESC,
        name_extractor=self._build_name_extractor()
    )
</execute_type4>

<helper_methods>
def _boolean_check_logic(self, parsed_data: Dict[str, Any]) -> tuple:
    """
    Boolean Check Logic (Type1/4 共享)
    
    核心业务逻辑：检查 netlist/SPEF 版本元数据是否存在
    
    Returns:
        tuple: (found_items, missing_items, extra_items)
    """
    netlist_info = parsed_data.get('netlist_info', {})
    spef_info = parsed_data.get('spef_info', {})
    errors = parsed_data.get('errors', [])
    
    found_items = {}
    missing_items = {}
    extra_items = {}
    
    # Check netlist version metadata
    netlist_status = netlist_info.get('status', 'Not Found')
    if netlist_status == 'Success':
        if netlist_info.get('path'):
            # File exists and accessible
            metadata = self._metadata.get('netlist_success', {})
            found_items["Netlist Version"] = {
                'line_number': metadata.get('line_number', 0),
                'file_path': metadata.get('file_path', ''),
                'version': netlist_info.get('version', 'Unknown'),
                'date': netlist_info.get('date', 'Unknown'),
                'path': netlist_info.get('path', 'Unknown')
            }
        elif netlist_info.get('relative_path'):
            # ⚠️ CRITICAL: Path found but file not accessible → still count as found!
            metadata = self._metadata.get('netlist_success', {})
            found_items["Netlist Version"] = {
                'line_number': metadata.get('line_number', 0),
                'file_path': metadata.get('file_path', ''),
                'note': 'found in log, file not accessible',
                'path': netlist_info['relative_path']
            }
    else:
        missing_items["Netlist Version"] = {
            'line_number': 0,
            'file_path': '',
            'reason': f"Status: {netlist_status}"
        }
    
    # Check SPEF version metadata
    spef_status = spef_info.get('status', 'Not Found')
    if spef_status == 'Success':
        if spef_info.get('path'):
            metadata = self._metadata.get('spef_success', {})
            found_items["SPEF Version"] = {
                'line_number': metadata.get('line_number', 0),
                'file_path': metadata.get('file_path', ''),
                'version': spef_info.get('version', 'Unknown'),
                'date': spef_info.get('date', 'Unknown'),
                'program': spef_info.get('program', 'Unknown'),
                'path': spef_info.get('path', 'Unknown')
            }
    elif spef_status == 'Skipped':
        # ⚠️ CRITICAL: SPEF skipped → missing_items (not extra_items!)
        metadata = self._metadata.get('spef_skipped', {})
        skip_reason = spef_info.get('skip_reason', 'SPEF reading was skipped')
        missing_items["SPEF Reading was skipped"] = {
            'line_number': metadata.get('line_number', 0),
            'file_path': metadata.get('file_path', ''),
            'reason': skip_reason
        }
    else:
        missing_items["SPEF Version"] = {
            'line_number': 0,
            'file_path': '',
            'reason': f"Status: {spef_status}"
        }
    
    # ⚠️ CRITICAL: Errors as extra_items (don't affect is_pass)
    for error in errors:
        if not any(e in error for e in ["SPEF reading was skipped"]):
            extra_items[f"Error: {error}"] = {
                'line_number': 0,
                'file_path': '',
                'reason': 'Unexpected error during parsing'
            }
    
    return found_items, missing_items, extra_items


def _pattern_check_logic(self, parsed_data: Dict[str, Any]) -> tuple:
    """
    Pattern Check Logic (Type2/3 共享)
    
    核心业务逻辑：在版本元数据中搜索 pattern_items
    
    Returns:
        tuple: (found_items, missing_items, extra_items)
    """
    # Get pattern_items
    requirements = self.item_data.get('requirements', {})
    pattern_items = requirements.get('pattern_items', [])
    
    netlist_info = parsed_data.get('netlist_info', {})
    spef_info = parsed_data.get('spef_info', {})
    
    found_items = {}
    missing_items = {}
    extra_items = {}
    
    # Build searchable content from version metadata
    searchable_content = []
    
    # Netlist metadata
    if netlist_info.get('version'):
        searchable_content.append({
            'text': f"Netlist Version: {netlist_info['version']}",
            'source': 'netlist',
            'metadata': netlist_info
        })
    if netlist_info.get('date'):
        searchable_content.append({
            'text': f"Netlist Date: {netlist_info['date']}",
            'source': 'netlist',
            'metadata': netlist_info
        })
    
    # SPEF metadata
    if spef_info.get('version'):
        searchable_content.append({
            'text': f"SPEF Version: {spef_info['version']}",
            'source': 'spef',
            'metadata': spef_info
        })
    if spef_info.get('date'):
        searchable_content.append({
            'text': f"SPEF Date: {spef_info['date']}",
            'source': 'spef',
            'metadata': spef_info
        })
    if spef_info.get('program'):
        searchable_content.append({
            'text': f"SPEF Program: {spef_info['program']}",
            'source': 'spef',
            'metadata': spef_info
        })
    
    # Match patterns
    for pattern in pattern_items:
        matched = False
        for content in searchable_content:
            if self._match_pattern_wildcard(content['text'], pattern):
                source = content['source']
                metadata = content['metadata']
                meta_tracking = self._metadata.get(f'{source}_success', {})
                
                found_items[pattern] = {
                    'line_number': meta_tracking.get('line_number', 0),
                    'file_path': meta_tracking.get('file_path', ''),
                    'matched': content['text'],
                    'source': source,
                    'version': metadata.get('version', ''),
                    'date': metadata.get('date', '')
                }
                matched = True
                break
        
        if not matched:
            missing_items[pattern] = {
                'line_number': 0,
                'file_path': '',
                'reason': f"Pattern '{pattern}' not found in version metadata"
            }
    
    # Check SPEF skip status - add as extra_item if skipped (Type2/3 specific)
    if spef_info.get('status') == 'Skipped':
        metadata = self._metadata.get('spef_skipped', {})
        skip_reason = spef_info.get('skip_reason', 'SPEF reading was skipped')
        extra_items["SPEF Reading was skipped"] = {
            'line_number': metadata.get('line_number', 0),
            'file_path': metadata.get('file_path', ''),
            'reason': skip_reason
        }
    
    return found_items, missing_items, extra_items


def _parse_sta_log(self, sta_log: Path) -> Dict[str, Any]:
    """
    Parse STA log to extract netlist/SPEF file paths and metadata.
    
    Returns:
        Dict with netlist_info, spef_info, flow_step
    """
    result = {
        'netlist_info': {'status': 'Not Found'},
        'spef_info': {'status': 'Not Found'},
        'flow_step': 'Unknown'
    }
    
    try:
        with open(sta_log, 'r', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                # Extract netlist path
                if 'read_netlist' in line:
                    match = re.search(r'read_netlist\s+(\S+)', line)
                    if match:
                        netlist_path_str = match.group(1)
                        netlist_path = Path(netlist_path_str)
                        if netlist_path.exists():
                            result['netlist_info']['path'] = str(netlist_path.resolve())
                        else:
                            # ⚠️ CRITICAL: Path found but file doesn't exist → save as relative_path
                            # Don't set status here! Wait for success indicator.
                            result['netlist_info']['relative_path'] = netlist_path_str
                
                # Netlist reading confirmation
                elif 'Reading verilog netlist' in line:
                    match = re.search(r"Reading verilog netlist '([^']+)'", line)
                    if match and 'path' not in result['netlist_info']:
                        netlist_path = Path(match.group(1))
                        if netlist_path.exists():
                            result['netlist_info']['path'] = str(netlist_path.resolve())
                
                # Netlist success indicator
                elif '*** Netlist is unique' in line or 'Netlist is unique' in line:
                    result['netlist_info']['status'] = 'Success'
                    self._metadata['netlist_success'] = {
                        'line_number': line_num,
                        'file_path': str(sta_log)
                    }
                
                # Top level cell
                elif 'Top level cell is' in line:
                    match = re.search(r'Top level cell is (\w+)\.', line)
                    if match:
                        result['netlist_info']['top_module'] = match.group(1)
                
                # Flow step
                elif '#@ flow_step:' in line:
                    match = re.search(r'#@ flow_step:(\w+)', line)
                    if match:
                        result['flow_step'] = match.group(1)
                
                # SPEF file path
                elif 'read_spef' in line:
                    match = re.search(r'read_spef\s+(\S+)', line)
                    if match:
                        spef_path = Path(match.group(1))
                        if spef_path.exists():
                            result['spef_info']['path'] = str(spef_path.resolve())
                            result['spef_info']['status'] = 'Success'
                            self._metadata['spef_success'] = {
                                'line_number': line_num,
                                'file_path': str(sta_log)
                            }
                
                # SPEF skipped
                elif '[INFO]' in line and 'Skipping SPEF reading' in line:
                    match = re.search(r'\[INFO\]\s*Skipping SPEF reading(.+)', line)
                    skip_reason = match.group(1).strip() if match else 'SPEF reading was skipped'
                    result['spef_info']['status'] = 'Skipped'
                    result['spef_info']['skip_reason'] = skip_reason
                    self._metadata['spef_skipped'] = {
                        'line_number': line_num,
                        'file_path': str(sta_log)
                    }
    
    except Exception as e:
        raise Exception(f"Failed to parse STA log: {str(e)}")
    
    return result


def _parse_netlist_file(self, netlist_path: Path) -> Dict[str, Any]:
    """
    Parse netlist file header to extract version metadata.
    
    Returns:
        Dict with version, date, tool info
    """
    result = {}
    
    try:
        with open(netlist_path, 'r', errors='ignore') as f:
            # Only read first 100 lines (header)
            for line_num, line in enumerate(f, 1):
                if line_num > 100:
                    break
                
                # Tool version
                if '// Generated by' in line or '//Generated by' in line:
                    match = re.search(r'//\s*Generated by[:\s]+([^\n]+)', line)
                    if match:
                        result['version'] = match.group(1).strip()
                
                # Generation date
                elif '// Date' in line or '//Date' in line:
                    match = re.search(r'//\s*Date[:\s]+([^\n]+)', line)
                    if match:
                        result['date'] = match.group(1).strip()
                
                # Version comment
                elif '// Version' in line or '//Version' in line:
                    match = re.search(r'//\s*Version[:\s]+([^\n]+)', line)
                    if match and 'version' not in result:
                        result['version'] = match.group(1).strip()
                
                # Module name
                elif line.strip().startswith('module '):
                    match = re.search(r'module\s+(\w+)\s*[#(;]', line)
                    if match:
                        result['top_module'] = match.group(1)
                        break  # Stop after finding module
    
    except Exception as e:
        raise Exception(f"Failed to parse netlist file: {str(e)}")
    
    return result


def _parse_spef_file(self, spef_path: Path) -> Dict[str, Any]:
    """
    Parse SPEF file header to extract version metadata (IEEE 1481 standard).
    
    Returns:
        Dict with version, date, program, vendor, etc.
    """
    result = {}
    
    try:
        with open(spef_path, 'r', errors='ignore') as f:
            # Only read first 100 lines (header)
            for line_num, line in enumerate(f, 1):
                if line_num > 100:
                    break
                
                # *VERSION
                if line.startswith('*VERSION'):
                    match = re.search(r'\*VERSION\s+"([^"]+)"', line)
                    if match:
                        result['version'] = match.group(1)
                
                # *DATE
                elif line.startswith('*DATE'):
                    match = re.search(r'\*DATE\s+"([^"]+)"', line)
                    if match:
                        result['date'] = match.group(1)
                
                # *PROGRAM
                elif line.startswith('*PROGRAM'):
                    match = re.search(r'\*PROGRAM\s+"([^"]+)"', line)
                    if match:
                        result['program'] = match.group(1)
                
                # *VENDOR
                elif line.startswith('*VENDOR'):
                    match = re.search(r'\*VENDOR\s+"([^"]+)"', line)
                    if match:
                        result['vendor'] = match.group(1)
                
                # *DESIGN
                elif line.startswith('*DESIGN'):
                    match = re.search(r'\*DESIGN\s+"([^"]+)"', line)
                    if match:
                        result['design'] = match.group(1)
                
                # *SPEF (format version)
                elif line.startswith('*SPEF'):
                    match = re.search(r'\*SPEF\s+"([^"]+)"', line)
                    if match:
                        result['spef_standard'] = match.group(1)
    
    except Exception as e:
        raise Exception(f"Failed to parse SPEF file: {str(e)}")
    
    return result


def _match_pattern_wildcard(self, text: str, pattern: str) -> bool:
    """
    Match pattern with wildcard support (* matches any characters).
    
    Args:
        text: Text to search in
        pattern: Pattern with optional wildcards (e.g., "Generated on:*2025*")
    
    Returns:
        bool: True if pattern matches
    """
    # Convert wildcard pattern to regex
    # Escape special regex chars except *
    regex_pattern = re.escape(pattern).replace(r'\*', '.*')
    regex_pattern = f'^{regex_pattern}$'
    
    try:
        return bool(re.search(regex_pattern, text, re.IGNORECASE))
    except Exception:
        # Fallback to simple substring match
        return pattern.replace('*', '').lower() in text.lower()
</helper_methods>
```