```xml
<thoughts>
1. **检查目标**: 验证 netlist/SPEF 文件的版本信息是否正确
   - 从 STA log 中提取 netlist/SPEF 文件路径
   - 解析 netlist 文件头部的工具版本和生成日期
   - 解析 SPEF 文件的 *VERSION, *DATE, *PROGRAM 等元数据
   - 支持 SPEF 跳过场景（synthesis 阶段）

2. **数据流**: STA_Log → 提取文件路径 → 解析 Netlist/SPEF → 提取版本信息 → 验证
   - STA log 包含 read_netlist 命令和文件路径
   - Netlist (Verilog) 头部包含 "// Generated by Cadence Genus..." 等信息
   - SPEF 包含 IEEE 1481 标准的 *VERSION, *DATE 等字段
   - 需要处理 gzip 压缩文件 (.gz)

3. **辅助方法**:
   - `_parse_sta_log()`: 解析 STA log，提取 netlist/SPEF 路径和状态
   - `_parse_netlist_version()`: 解析 netlist 文件提取版本信息
   - `_parse_spef_version()`: 解析 SPEF 文件提取版本信息
   - `_read_file_content()`: 读取文件内容（支持 gzip）
   - `_resolve_relative_path()`: 解析相对路径为绝对路径
   - `_match_pattern()`: 模式匹配（支持通配符）
   - `_build_name_extractor()`: 格式化输出名称

4. **Pass/Fail 逻辑**:
   - Type 1: Boolean - netlist/SPEF 加载成功即 PASS，SPEF 跳过算 FAIL
   - Type 2: Value - 匹配 pattern_items 中的版本模式，计数匹配数量
   - Type 3: Value + Waiver - 匹配模式 + waiver 逻辑
   - Type 4: Boolean + Waiver - 存在检查 + waiver 逻辑

5. **Metadata 追踪**:
   - `self._metadata['netlist_cmd']`: netlist 命令行号
   - `self._metadata['netlist_success']`: netlist 成功标记行号
   - `self._metadata['spef_step_end']`: SPEF 读取完成行号
   - `self._metadata['spef_skipped']`: SPEF 跳过信息
   - 在 DetailItem 中使用 metadata 的 line_number 和 file_path
</thoughts>

<class_name>
NetlistSpefVersionChecker
</class_name>

<class_constants>
FOUND_DESC = "Netlist/SPEF files loaded successfully"
MISSING_DESC = "Netlist/SPEF loading issues"
WAIVED_DESC = "Version check waived for current flow stage"
FOUND_REASON = "Status: Success"
MISSING_REASON = "File loading failed"
WAIVED_BASE_REASON = "Version check waived"
EXTRA_REASON = "Design has no spef/netlist file or unexpected error"
UNUSED_WAIVER_REASON = "Waiver defined but not applied (version metadata found)"
</class_constants>

<name_extractor_method>
def _build_name_extractor(self):
    """返回 name_extractor 函数，用于格式化输出"""
    def extract_name(name: str, metadata: Any) -> str:
        if isinstance(metadata, dict):
            version = metadata.get('version', '')
            date = metadata.get('date', '')
            note = metadata.get('note', '')
            matched = metadata.get('matched', '')
            reason = metadata.get('reason', '')
            
            if version and date:
                return f"{name}, Version: {version}, Date: {date}"
            elif note:
                return f"{name} ({note})"
            elif matched:
                return f"{name}: {matched}"
            elif reason:
                return f"{name}: {reason}"
        return name
    return extract_name
</name_extractor_method>

<parse_method>
def _parse_input_files(self) -> Dict[str, Any]:
    """
    Parse input files to extract netlist and SPEF version information.
    
    Returns:
        Dict with keys:
        - netlist_info: Dict with netlist metadata
        - spef_info: Dict with SPEF metadata
        - errors: List of error messages
    """
    # Initialize metadata storage
    self._metadata = {}
    
    # Parse STA log first
    sta_info = self._parse_sta_log()
    
    netlist_info = {}
    spef_info = {}
    errors = list(sta_info.get('errors', []))
    
    # Parse netlist file if found
    if sta_info.get('netlist_path'):
        netlist_path = sta_info['netlist_path']
        netlist_info = self._parse_netlist_version(netlist_path)
        netlist_info['path'] = str(netlist_path)
        netlist_info['status'] = sta_info.get('netlist_status', 'Unknown')
        
        if not netlist_info.get('version'):
            errors.append(f"Failed to extract version from netlist: {netlist_path.name}")
    elif sta_info.get('netlist_relative_path'):
        # Netlist path found but file doesn't exist
        netlist_info['relative_path'] = sta_info['netlist_relative_path']
        netlist_info['status'] = sta_info.get('netlist_status', 'Unknown')
        netlist_info['note'] = 'File path found in log but actual file not accessible'
    else:
        errors.append("Netlist file path not found in STA log")
    
    # Parse SPEF file if found
    if sta_info.get('spef_path'):
        spef_path = sta_info['spef_path']
        spef_info = self._parse_spef_version(spef_path)
        spef_info['path'] = str(spef_path)
        spef_info['status'] = sta_info.get('spef_status', 'Unknown')
        
        if not spef_info.get('version'):
            errors.append(f"Failed to extract version from SPEF: {spef_path.name}")
    else:
        # SPEF might be intentionally skipped or not found
        spef_status = sta_info.get('spef_status', 'Not Found')
        spef_info['status'] = spef_status
        if spef_status == 'Skipped':
            # Get skip reason from metadata
            metadata = self._metadata.get('spef_skipped', {})
            skip_reason = metadata.get('reason', 'SPEF reading was skipped')
            # Remove [INFO] prefix if present
            skip_reason = skip_reason.replace('[INFO] ', '')
            spef_info['skip_reason'] = skip_reason
        elif spef_status == 'Not Found':
            if 'spef_step_end' not in self._metadata:
                errors.append("SPEF file path not found in STA log")
    
    return {
        'netlist_info': netlist_info,
        'spef_info': spef_info,
        'errors': errors
    }
</parse_method>

<execute_type1>
def _execute_type1(self, parsed_data: Dict[str, Any]) -> CheckResult:
    """
    Type 1: Boolean check with automatic waiver.value=0 support
    
    Check if netlist and SPEF are loaded successfully.
    "Skipping SPEF reading" counts as FAIL.
    
    Waiver Logic (Automatic via build_complete_output):
    - waiver.value = 0: Auto-convert FAIL→INFO, force PASS [WAIVED_AS_INFO]
    - waiver.value = N/A: Normal mode
    """
    netlist_info = parsed_data.get('netlist_info', {})
    spef_info = parsed_data.get('spef_info', {})
    errors = parsed_data.get('errors', [])
    
    found_items = {}
    missing_items = []
    
    # Check netlist
    netlist_status = netlist_info.get('status', 'Not Found')
    if netlist_status == 'Success':
        if netlist_info.get('path'):
            netlist_path = netlist_info.get('path', 'Unknown')
            version_str = netlist_info.get('version', 'Unknown')
            date_str = netlist_info.get('full_timestamp', netlist_info.get('date', 'Unknown'))
            
            metadata = self._metadata.get('netlist_success', {})
            item_name = f"Netlist: {netlist_path}"
            found_items[item_name] = {
                'line_number': metadata.get('line_number', 0),
                'file_path': metadata.get('file_path', ''),
                'version': version_str,
                'date': date_str
            }
        elif netlist_info.get('relative_path'):
            netlist_rel_path = netlist_info['relative_path']
            metadata = self._metadata.get('netlist_success', {})
            item_name = f"Netlist: {netlist_rel_path}"
            found_items[item_name] = {
                'line_number': metadata.get('line_number', 0),
                'file_path': metadata.get('file_path', ''),
                'note': 'found in log, file not accessible'
            }
    else:
        missing_items.append(f"Netlist (Status: {netlist_status})")
    
    # Check SPEF
    spef_status = spef_info.get('status', 'Not Found')
    if spef_status == 'Success':
        if spef_info.get('path'):
            spef_path = spef_info.get('path', 'Unknown')
            version_str = spef_info.get('version', 'Unknown')
            date_str = spef_info.get('date', 'Unknown')
            
            metadata = self._metadata.get('spef_step_end', {})
            item_name = f"SPEF: {spef_path}"
            found_items[item_name] = {
                'line_number': metadata.get('line_number', 0),
                'file_path': metadata.get('file_path', ''),
                'version': version_str,
                'date': date_str
            }
    elif spef_status == 'Skipped':
        metadata = self._metadata.get('spef_skipped', {})
        skip_reason = metadata.get('reason', 'SPEF reading was skipped')
        skip_reason = skip_reason.replace('[INFO] ', '')
        missing_items.append(f"SPEF Reading was skipped ({skip_reason})")
    else:
        missing_items.append(f"SPEF (Status: {spef_status})")
    
    # Add other errors
    for error in errors:
        if not any(e in error for e in ["SPEF reading was skipped"]):
            missing_items.append(f"Error: {error}")
    
    return self.build_complete_output(
        found_items=found_items,
        missing_items=missing_items,
        found_desc=self.FOUND_DESC,
        missing_desc=self.MISSING_DESC,
        name_extractor=self._build_name_extractor()
    )
</execute_type1>

<execute_type2>
def _execute_type2(self, parsed_data: Dict[str, Any]) -> CheckResult:
    """
    Type 2: Value comparison with automatic waiver.value=0 support
    
    Match required items in pattern_items against netlist/SPEF content.
    Expected value = number of items that should be found (should match pattern_items count).
    
    Waiver Logic (Automatic via build_complete_output):
    - waiver.value = 0: Auto-convert FAIL/WARN→INFO, force PASS [WAIVED_AS_INFO]
    - waiver.value = N/A: Normal mode
    """
    netlist_info = parsed_data.get('netlist_info', {})
    spef_info = parsed_data.get('spef_info', {})
    errors = parsed_data.get('errors', [])
    
    requirements = self.item_data.get('requirements', {})
    pattern_items = requirements.get('pattern_items', []) if requirements else []
    
    found_items = {}
    missing_items = []
    extra_items = {}  # Items not in pattern (SPEF issues, etc.)
    
    # Collect all content to search
    all_content = []
    
    # Add netlist version info
    if netlist_info.get('tool'):
        all_content.append(f"Tool: {netlist_info['tool']}")
    if netlist_info.get('version'):
        all_content.append(f"Genus Synthesis Solution {netlist_info['version']}")
    if netlist_info.get('full_timestamp'):
        all_content.append(f"Generated on: {netlist_info['full_timestamp']}")
    
    # Add SPEF version info
    if spef_info.get('program'):
        all_content.append(f"Program: {spef_info['program']}")
    if spef_info.get('version'):
        all_content.append(f"VERSION {spef_info['version']}")
    if spef_info.get('date'):
        all_content.append(f"DATE {spef_info['date']}")
    
    # Match patterns against content
    matched_patterns = set()
    for pattern in pattern_items:
        found = False
        matched_content = None
        for content in all_content:
            if self._match_pattern(content, [pattern]):
                found = True
                matched_content = content
                break
        
        if found:
            matched_patterns.add(pattern)
            # Build found_items with file/version metadata
            if netlist_info.get('path') and ('Genus' in pattern or 'Generated on' in pattern):
                metadata = self._metadata.get('netlist_success', {})
                found_items[pattern] = {
                    'line_number': metadata.get('line_number', 0),
                    'file_path': metadata.get('file_path', ''),
                    'matched': matched_content
                }
            elif spef_info.get('path') and ('Quantus' in pattern or 'DATE' in pattern or 'VERSION' in pattern):
                metadata = self._metadata.get('spef_step_end', {})
                found_items[pattern] = {
                    'line_number': metadata.get('line_number', 0),
                    'file_path': metadata.get('file_path', ''),
                    'matched': matched_content
                }
            else:
                found_items[pattern] = {
                    'line_number': 0,
                    'file_path': '',
                    'matched': matched_content
                }
    
    # Find unmatched patterns
    missing_items = [p for p in pattern_items if p not in matched_patterns]
    
    # Check SPEF skip status - add as extra_item if skipped
    if spef_info.get('status') == 'Skipped':
        metadata = self._metadata.get('spef_skipped', {})
        skip_reason = spef_info.get('skip_reason', 'SPEF reading was skipped')
        extra_items["SPEF Reading was skipped"] = {
            'line_number': metadata.get('line_number', 0),
            'file_path': metadata.get('file_path', ''),
            'reason': skip_reason
        }
    
    # Add other errors as extra items
    for error in errors:
        if not any(e in error for e in ["SPEF reading was skipped"]):
            extra_items[f"Error: {error}"] = {
                'line_number': 0,
                'file_path': '',
                'reason': 'Unexpected error'
            }
    
    return self.build_complete_output(
        found_items=found_items,
        missing_items=missing_items,
        extra_items=extra_items,
        found_desc="Netlist/SPEF version is correct",
        missing_desc="Netlist/SPEF version isn't correct",
        extra_desc="Design has no spef/netlist file",
        name_extractor=self._build_name_extractor()
    )
</execute_type2>

<execute_type3>
def _execute_type3(self, parsed_data: Dict[str, Any]) -> CheckResult:
    """
    Type 3: Value check with waiver logic
    
    Match pattern_items and apply waiver logic for missing items.
    """
    netlist_info = parsed_data.get('netlist_info', {})
    spef_info = parsed_data.get('spef_info', {})
    errors = parsed_data.get('errors', [])
    
    # Get configuration
    requirements = self.item_data.get('requirements', {})
    pattern_items = requirements.get('pattern_items', []) if requirements else []
    waive_items_dict = self.get_waive_items_with_reasons()
    
    # Collect all content to search
    all_content = []
    if netlist_info.get('tool'):
        all_content.append(f"Tool: {netlist_info['tool']}")
    if netlist_info.get('version'):
        all_content.append(f"Genus Synthesis Solution {netlist_info['version']}")
    if netlist_info.get('full_timestamp'):
        all_content.append(f"Generated on: {netlist_info['full_timestamp']}")
    if spef_info.get('program'):
        all_content.append(f"Program: {spef_info['program']}")
    if spef_info.get('version'):
        all_content.append(f"VERSION {spef_info['version']}")
    if spef_info.get('date'):
        all_content.append(f"DATE {spef_info['date']}")
    
    # Classify: found / missing / waived
    matched_patterns = set()
    waived_patterns = []
    used_waivers = set()
    
    for pattern in pattern_items:
        found = False
        for content in all_content:
            if self._match_pattern(content, [pattern]):
                found = True
                matched_patterns.add(pattern)
                break
        
        if not found:
            # Check if waived using word-level matching
            is_waived, waiver_key, waiver_reason = self.is_item_waived_word_level(pattern, waive_items_dict)
            if is_waived:
                waived_patterns.append(pattern)
                used_waivers.add(waiver_key)
    
    missing_patterns = [p for p in pattern_items 
                        if p not in matched_patterns and p not in waived_patterns]
    unused_waivers = [w for w in waive_items_dict if w not in used_waivers]
    
    # Build details
    details = []
    
    for pattern in matched_patterns:
        # Find matched content
        matched_content = None
        for content in all_content:
            if self._match_pattern(content, [pattern]):
                matched_content = content
                break
        
        # Determine metadata source
        if netlist_info.get('path') and ('Genus' in pattern or 'Generated on' in pattern):
            metadata = self._metadata.get('netlist_success', {})
        elif spef_info.get('path') and ('Quantus' in pattern or 'DATE' in pattern or 'VERSION' in pattern):
            metadata = self._metadata.get('spef_step_end', {})
        else:
            metadata = {}
        
        details.append(DetailItem(
            severity=Severity.INFO,
            name=pattern,
            line_number=metadata.get('line_number', 0),
            file_path=metadata.get('file_path', ''),
            reason=f"Version pattern matched: {matched_content}"
        ))
    
    for pattern in waived_patterns:
        is_waived, waiver_key, waiver_reason = self.is_item_waived_word_level(pattern, waive_items_dict)
        reason = f"Required pattern not found[WAIVER]"
        if waiver_reason:
            reason = f"Required pattern not found: {waiver_reason}[WAIVER]"
        details.append(DetailItem(
            severity=Severity.INFO,
            name=pattern,
            line_number=0,
            file_path='',
            reason=reason
        ))
    
    for pattern in missing_patterns:
        details.append(DetailItem(
            severity=Severity.FAIL,
            name=pattern,
            line_number=0,
            file_path='',
            reason="Required pattern not found"
        ))
    
    for waiver_key in unused_waivers:
        details.append(DetailItem(
            severity=Severity.WARN,
            name=waiver_key,
            line_number=0,
            file_path='',
            reason="Waiver not used[WAIVER]"
        ))
    
    # Build groups
    error_groups = None
    info_groups = None
    warn_groups = None
    
    if missing_patterns:
        error_groups = {"ERROR01": {"description": "Netlist/SPEF version isn't correct", "items": missing_patterns}}
    if matched_patterns:
        info_groups = {"INFO01": {"description": "Netlist/SPEF version is correct", "items": list(matched_patterns)}}
    if unused_waivers:
        warn_groups = {"WARN01": {"description": "Unused waivers", "items": unused_waivers}}
    
    return create_check_result(
        value=len(matched_patterns),
        is_pass=len(missing_patterns) == 0,
        has_pattern_items=True,
        has_waiver_value=True,
        details=details,
        error_groups=error_groups,
        info_groups=info_groups,
        warn_groups=warn_groups,
        item_desc=self.item_desc
    )
</execute_type3>

<execute_type4>
def _execute_type4(self, parsed_data: Dict[str, Any]) -> CheckResult:
    """
    Type 4: Boolean check with waiver logic
    
    Check if version metadata exists, with waiver support for flow stages.
    """
    netlist_info = parsed_data.get('netlist_info', {})
    spef_info = parsed_data.get('spef_info', {})
    errors = parsed_data.get('errors', [])
    
    waive_items_dict = self.get_waive_items_with_reasons()
    
    found_items = {}
    missing_items = []
    waived_items = []
    used_waivers = set()
    
    # Check netlist
    netlist_status = netlist_info.get('status', 'Not Found')
    if netlist_status == 'Success':
        if netlist_info.get('path'):
            netlist_path = netlist_info.get('path', 'Unknown')
            version_str = netlist_info.get('version', 'Unknown')
            date_str = netlist_info.get('full_timestamp', netlist_info.get('date', 'Unknown'))
            
            metadata = self._metadata.get('netlist_success', {})
            item_name = f"Netlist: {netlist_path}"
            found_items[item_name] = {
                'line_number': metadata.get('line_number', 0),
                'file_path': metadata.get('file_path', ''),
                'version': version_str,
                'date': date_str
            }
    else:
        netlist_issue = f"Netlist (Status: {netlist_status})"
        is_waived, waiver_key, waiver_reason = self.is_item_waived_word_level(netlist_issue, waive_items_dict)
        if is_waived:
            waived_items.append(netlist_issue)
            used_waivers.add(waiver_key)
        else:
            missing_items.append(netlist_issue)
    
    # Check SPEF
    spef_status = spef_info.get('status', 'Not Found')
    if spef_status == 'Success':
        if spef_info.get('path'):
            spef_path = spef_info.get('path', 'Unknown')
            version_str = spef_info.get('version', 'Unknown')
            date_str = spef_info.get('date', 'Unknown')
            
            metadata = self._metadata.get('spef_step_end', {})
            item_name = f"SPEF: {spef_path}"
            found_items[item_name] = {
                'line_number': metadata.get('line_number', 0),
                'file_path': metadata.get('file_path', ''),
                'version': version_str,
                'date': date_str
            }
    elif spef_status == 'Skipped':
        metadata = self._metadata.get('spef_skipped', {})
        skip_reason = metadata.get('reason', 'SPEF reading was skipped')
        skip_reason = skip_reason.replace('[INFO] ', '')
        spef_issue = f"SPEF Reading was skipped ({skip_reason})"
        
        # Check if SPEF skip is waived
        is_waived, waiver_key, waiver_reason = self.is_item_waived_word_level(spef_issue, waive_items_dict)
        if is_waived:
            waived_items.append(spef_issue)
            used_waivers.add(waiver_key)
        else:
            missing_items.append(spef_issue)
    else:
        spef_issue = f"SPEF (Status: {spef_status})"
        is_waived, waiver_key, waiver_reason = self.is_item_waived_word_level(spef_issue, waive_items_dict)
        if is_waived:
            waived_items.append(spef_issue)
            used_waivers.add(waiver_key)
        else:
            missing_items.append(spef_issue)
    
    unused_waivers = [w for w in waive_items_dict if w not in used_waivers]
    
    return self.build_complete_output(
        found_items=found_items,
        missing_items=missing_items,
        waived_items=waived_items,
        unused_waivers=unused_waivers,
        found_desc=self.FOUND_DESC,
        missing_desc=self.MISSING_DESC,
        waived_desc=self.WAIVED_DESC,
        name_extractor=self._build_name_extractor()
    )
</execute_type4>

<helper_methods>
def _parse_sta_log(self) -> Dict[str, Any]:
    """
    Parse STA log file to extract netlist/SPEF information.
    
    Returns:
        Dict with keys:
        - netlist_path: Path to netlist file
        - netlist_status: Success/Failed
        - spef_path: Path to SPEF file (if any)
        - spef_status: Success/Failed/Skipped
        - errors: List of error messages
    """
    sta_info = {
        'netlist_path': None,
        'netlist_status': 'Not Found',
        'spef_path': None,
        'spef_status': 'Not Found',
        'errors': [],
        'warnings': []
    }
    
    # Validate input_files configuration
    if not self.item_data or 'input_files' not in self.item_data:
        sta_info['errors'].append("No input_files specified in configuration")
        return sta_info
    
    input_files = self.item_data['input_files']
    
    if isinstance(input_files, str):
        input_files = [input_files]
    
    if not input_files:
        sta_info['errors'].append("input_files list is empty")
        return sta_info
    
    for file_path_str in input_files:
        file_path = Path(file_path_str)
        
        if not file_path.exists():
            sta_info['errors'].append(f"STA log file not found: {file_path_str}")
            continue
        
        sta_log_dir = file_path.parent
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
        except Exception as e:
            sta_info['errors'].append(f"Failed to read STA log: {e}")
            continue
        
        for line_num, line in enumerate(lines, 1):
            # Extract netlist file path
            if 'read_netlist' in line and '<CMD>' in line:
                match = re.search(r'read_netlist\s+(\S+)', line)
                if match:
                    netlist_rel_path = match.group(1)
                    netlist_abs_path = self._resolve_relative_path(netlist_rel_path, sta_log_dir)
                    if netlist_abs_path:
                        sta_info['netlist_path'] = netlist_abs_path
                    else:
                        sta_info['netlist_relative_path'] = netlist_rel_path
                    
                    self._metadata['netlist_cmd'] = {
                        'line_number': line_num,
                        'file_path': str(file_path),
                        'relative_path': netlist_rel_path
                    }
            
            # Reading verilog netlist
            elif 'Reading verilog netlist' in line:
                match = re.search(r"Reading verilog netlist\s+'([^']+)'", line)
                if match:
                    netlist_rel_path = match.group(1)
                    if not sta_info.get('netlist_path'):
                        netlist_abs_path = self._resolve_relative_path(netlist_rel_path, sta_log_dir)
                        if netlist_abs_path:
                            sta_info['netlist_path'] = netlist_abs_path
                        else:
                            sta_info['netlist_relative_path'] = netlist_rel_path
                
                self._metadata['netlist_reading'] = {
                    'line_number': line_num,
                    'file_path': str(file_path)
                }
            
            # Check netlist success
            elif '*** Netlist is unique' in line or 'Netlist is unique' in line:
                sta_info['netlist_status'] = 'Success'
                self._metadata['netlist_success'] = {
                    'line_number': line_num,
                    'file_path': str(file_path)
                }
            
            # Check for SPEF reading
            elif 'Skipping SPEF reading' in line:
                sta_info['spef_status'] = 'Skipped'
                sta_info['errors'].append("SPEF reading was skipped")
                self._metadata['spef_skipped'] = {
                    'line_number': line_num,
                    'file_path': str(file_path),
                    'reason': line.strip()
                }
            
            # read_parasitics step
            elif 'Begin flow_step read_parasitics' in line:
                self._metadata['spef_step_begin'] = {
                    'line_number': line_num,
                    'file_path': str(file_path)
                }
            
            elif 'End flow_step read_parasitics' in line:
                self._metadata['spef_step_end'] = {
                    'line_number': line_num,
                    'file_path': str(file_path)
                }
                if sta_info['spef_status'] != 'Skipped':
                    sta_info['spef_status'] = 'Success'
            
            # Look for SPEF file path
            elif 'read_spef' in line.lower() or 'read_parasitics' in line.lower():
                match = re.search(r'([\w/\.\-]+\.spef(?:\.gz)?)', line, re.IGNORECASE)
                if match:
                    spef_rel_path = match.group(1)
                    spef_abs_path = self._resolve_relative_path(spef_rel_path, sta_log_dir)
                    if spef_abs_path:
                        sta_info['spef_path'] = spef_abs_path
            
            # Check for errors
            elif re.search(r'\b(error|failed)\b', line, re.IGNORECASE):
                if 'netlist' in line.lower() or 'spef' in line.lower():
                    sta_info['errors'].append(f"Line {line_num}: {line.strip()}")
    
    # Final status determination
    if sta_info.get('netlist_relative_path') or sta_info.get('netlist_path'):
        if 'netlist_success' in self._metadata:
            sta_info['netlist_status'] = 'Success'
        elif sta_info['netlist_status'] == 'Not Found':
            sta_info['netlist_status'] = 'Read Command Found'
    
    return sta_info

def _parse_netlist_version(self, netlist_path: Path) -> Dict[str, str]:
    """
    Parse netlist file to extract version information.
    
    Expected format:
    // Generated by Cadence Genus(TM) Synthesis Solution 23.15-s099_1
    // Generated on: Nov 18 2025 15:58:15 IST (Nov 18 2025 10:28:15 UTC)
    
    Returns:
        Dict with keys: tool, version, date, time
    """
    version_info = {
        'tool': '',
        'version': '',
        'date': '',
        'time': '',
        'full_timestamp': ''
    }
    
    if not netlist_path.exists():
        return version_info
    
    lines = self._read_file_content(netlist_path, max_lines=50)
    
    for line in lines:
        # Extract tool and version
        if 'Generated by' in line and 'Genus' in line:
            match = re.search(r'Synthesis Solution\s+([\d\.\-\w]+)', line)
            if match:
                version_info['tool'] = 'Cadence Genus Synthesis Solution'
                version_info['version'] = match.group(1)
        
        # Extract generation date
        elif 'Generated on:' in line:
            match = re.search(r'Generated on:\s+(\w+\s+\d+\s+\d+)\s+([\d:]+)', line)
            if match:
                version_info['date'] = match.group(1)
                version_info['time'] = match.group(2)
                version_info['full_timestamp'] = f"{match.group(1)} {match.group(2)}"
    
    return version_info

def _parse_spef_version(self, spef_path: Path) -> Dict[str, str]:
    """
    Parse SPEF file to extract version information.
    
    Expected format:
    *SPEF "IEEE 1481-1999"
    *DESIGN "design_name"
    *DATE "Tue Jun 10 14:16:48 2025"
    *VENDOR "Cadence Design Systems Inc"
    *PROGRAM "Cadence Quantus Extraction"
    *VERSION "23.1.0-p075 Tue Sep 26 09:27:40 PDT 2023"
    
    Returns:
        Dict with keys: design, date, vendor, program, version
    """
    version_info = {
        'design': '',
        'date': '',
        'vendor': '',
        'program': '',
        'version': '',
        'spef_standard': ''
    }
    
    if not spef_path.exists():
        return version_info
    
    lines = self._read_file_content(spef_path, max_lines=100)
    
    for line in lines:
        if line.startswith('*SPEF'):
            match = re.search(r'\*SPEF\s+"([^"]+)"', line)
            if match:
                version_info['spef_standard'] = match.group(1)
        
        elif line.startswith('*DESIGN'):
            match = re.search(r'\*DESIGN\s+"([^"]+)"', line)
            if match:
                version_info['design'] = match.group(1)
        
        elif line.startswith('*DATE'):
            match = re.search(r'\*DATE\s+"([^"]+)"', line)
            if match:
                version_info['date'] = match.group(1)
        
        elif line.startswith('*VENDOR'):
            match = re.search(r'\*VENDOR\s+"([^"]+)"', line)
            if match:
                version_info['vendor'] = match.group(1)
        
        elif line.startswith('*PROGRAM'):
            match = re.search(r'\*PROGRAM\s+"([^"]+)"', line)
            if match:
                version_info['program'] = match.group(1)
        
        elif line.startswith('*VERSION'):
            match = re.search(r'\*VERSION\s+"([^"]+)"', line)
            if match:
                version_info['version'] = match.group(1)
    
    return version_info

def _read_file_content(self, file_path: Path, max_lines: int = 100) -> List[str]:
    """
    Read file content, supporting both plain text and gzip compressed files.
    
    Args:
        file_path: Path to the file
        max_lines: Maximum number of lines to read from start
        
    Returns:
        List of lines from the file
    """
    if not file_path.exists():
        return []
    
    try:
        # Check if file is gzipped
        if file_path.suffix == '.gz':
            try:
                with gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore') as f:
                    lines = []
                    for i, line in enumerate(f):
                        if i >= max_lines:
                            break
                        lines.append(line)
                    return lines
            except (gzip.BadGzipFile, OSError):
                # Not a real gzip file, fall through to read as plain text
                pass
        
        # Read as plain text file
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = []
            for i, line in enumerate(f):
                if i >= max_lines:
                    break
                lines.append(line)
            return lines
    except Exception as e:
        print(f"Warning: Failed to read {file_path}: {e}")
        return []

def _resolve_relative_path(self, relative_path: str, sta_log_dir: Path) -> Optional[Path]:
    """
    Resolve relative path from STA log to absolute path.
    
    Args:
        relative_path: Relative path from STA log
        sta_log_dir: Directory containing the STA log file
        
    Returns:
        Absolute path if exists, None otherwise
    """
    if not relative_path:
        return None
    
    try:
        # Clean path
        clean_path = relative_path.strip().strip('"').strip("'")
        
        # Resolve relative to STA log directory
        abs_path = (sta_log_dir / clean_path).resolve()
        
        if abs_path.exists():
            return abs_path
        
        # If not found, try from project root
        if hasattr(self, 'root') and self.root:
            abs_path = (self.root / clean_path).resolve()
            if abs_path.exists():
                return abs_path
    except Exception as e:
        print(f"Warning: Failed to resolve path {relative_path}: {e}")
    
    return None

def _match_pattern(self, text: str, patterns: List[str]) -> Optional[str]:
    """
    Check if text matches any pattern.
    
    Args:
        text: Text to check
        patterns: List of patterns (supports wildcards)
        
    Returns:
        Matched pattern if found, None otherwise
    """
    for pattern in patterns:
        try:
            # Convert wildcard to regex
            regex_pattern = pattern
            if '*' in pattern and not pattern.startswith('^'):
                regex_pattern = pattern.replace('*', '.*')
            
            if re.search(regex_pattern, text, re.IGNORECASE):
                return pattern
        except re.error:
            # Regex error, try exact match
            if pattern.lower() in text.lower():
                return pattern
    return None
</helper_methods>
```