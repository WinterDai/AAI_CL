# ğŸ“š å­¦ä¹ ææ–™ (Grounding Data)

## ğŸŒŸ Golden ä»£ç ç‰‡æ®µ (è¡¥å……å‚è€ƒ)

ä»¥ä¸‹æ˜¯æ¥è‡ª Golden ä»£ç çš„å…³é”®æ–¹æ³•å®ç°ï¼š

**_parse_input_files**:
```python
def _parse_input_files(self) -> Tuple[Dict[str, Any], Dict[str, Any], List[str]]:
        """
        Parse input files to extract netlist and SPEF version information.
        
        Returns:
            Tuple of (netlist_info, spef_info, error_list)
        """
        # Parse STA log first
        self._sta_log_info = self._parse_sta_log()
        
        netlist_info = {}
        spef_info = {}
        errors = list(self._sta_log_info.get('errors', []))
        
        # Parse netlist file if found
        if self._sta_log_info.get('netlist_path'):
            netlist_path = self._sta_log_info['netlist_path']
            netlist_info = self._parse_netlist_version(netlist_path)
            netlist_info['path'] = str(netlist_path)
            netlist_info['status'] = self._sta_log_info.get('netlist_status', 'Unknown')
            
            if not netlist_info.get('version'):
                errors.append(f"Failed to extract version from netlist: {netlist_path.name}")
        elif self._sta_log_info.get('netlist_relative_path'):
            # Netlist path found but file doesn't exist
            netlist_info['relative_path'] = self._sta_log_info['netlist_relative_path']
            netlist_info['status'] = self._sta_log_info.get('netlist_status', 'Unknown')
            netlist_info['note'] = 'File path found in log but actual file not accessible'
        else:
            errors.append("Netlist file path not found in STA log")
        
        # Parse SPEF file if found
        if self._sta_log_info.get('spef_path'):
            spef_path = self._sta_log_info['spef_path']
            spef_info = self._parse_spef_version(spef_path)
            spef_info['path'] = str(spef_path)
            spef_info['status'] = self._sta_log_info.get('spef_status', 'Unknown')
            
            if not spef_info.get('version'):
                errors.append(f"Failed to extract version from SPEF: {spef_path.name}")
        else:
            # SPEF might be intentionally skipped or not found
            spef_status = self._sta_log_info.get('spef_status', 'Not Found')
            spef_info['status'] = spef_status
            if spef_status == 'Skipped':
                # Get skip reason from metadata
                metadata = self._metadata.get('spef_skipped', {})
                skip_reason = metadata.get('reason', 'SPEF reading was skipped')
                # Remove [INFO] prefix if present
                skip_reason = skip_reason.replace('[INFO] ', '')
                spef_info['skip_reason'] = skip_reason
            elif spef_status == 'Not Found':
                if 'spef_step_end' not in self._metadata:
                    errors.append("SPEF file path not found in STA log")
        
        return netlist_info, spef_info, errors
```

**_execute_type1**:
```python
def _execute_type1(self, netlist_info: Dict[str, Any], spef_info: Dict[str, Any], 
                       errors: List[str]) -> CheckResult:
        """
        Type 1: Boolean check with automatic waiver.value=0 support
        
        Check if netlist and SPEF are loaded successfully.
        "Skipping SPEF reading" counts as FAIL.
        
        Waiver Logic (Automatic via build_complete_output):
        - waiver.value = 0: Auto-convert FAILâ†’INFO, force PASS [WAIVED_AS_INFO]
                           Auto-parse waive_items and output as INFO [WAIVED_INFO]
        - waiver.value = N/A: Normal mode
        """
        found_items = {}
        missing_items = []
        
        # Check netlist
        netlist_status = netlist_info.get('status', 'Not Found')
        if netlist_status == 'Success':
            if netlist_info.get('path'):
                netlist_path = netlist_info.get('path', 'Unknown')
                version_str = netlist_info.get('version', 'Unknown')
                date_str = netlist_info.get('full_timestamp', netlist_info.get('date', 'Unknown'))
                
                metadata = self._metadata.get('netlist_success', {})
                item_name = f"Netlist: {netlist_path}"
                found_items[item_name] = {
                    'line_number': metadata.get('line_number', ''),
                    'file_path': metadata.get('file_path', ''),
                    'version': version_str,
                    'date': date_str
                }
            elif netlist_info.get('relative_path'):
                netlist_rel_path = netlist_info['relative_path']
                metadata = self._metadata.get('netlist_success', {})
                item_name = f"Netlist: {netlist_rel_path}"
                found_items[item_name] = {
                    'line_number': metadata.get('line_number', ''),
                    'file_path': metadata.get('file_path', ''),
                    'note': 'found in log, file not accessible'
                }
        else:
            missing_items.append(f"Netlist (Status: {netlist_status})")
        
        # Check SPEF
        spef_status = spef_info.get('status', 'Not Found')
        if spef_status == 'Success':
            if spef_info.get('path'):
                spef_path = spef_info.get('path', 'Unknown')
                version_str = spef_info.get('version', 'Unknown')
                date_str = spef_info.get('date', 'Unknown')
                
                metadata = self._metadata.get('spef_step_end', {})
                item_name = f"SPEF: {spef_path}"
                found_items[item_name] = {
                    'line_number': metadata.get('line_number', ''),
                    'file_path': metadata.get('file_path', ''),
                    'version': version_str,
                    'date': date_str
                }
        elif spef_status == 'Skipped':
            metadata = self._metadata.get('spef_skipped', {})
            skip_reason = metadata.get('reason', 'SPEF reading was skipped')
            skip_reason = skip_reason.replace('[INFO] ', '')
            missing_items.append(f"SPEF Reading was skipped ({skip_reason})")
        else:
            missing_items.append(f"SPEF (Status: {spef_status})")
        
        # Add other errors
        for error in errors:
            if not any(e in error for e in ["SPEF reading was skipped"]):
                missing_items.append(f"Error: {error}")
        
        # Name extractor to format output
        def extract_name(name, metadata):
            if isinstance(metadata, dict):
                version = metadata.get('version', '')
                date = metadata.get('date', '')
                note = metadata.get('note', '')
                if version and date:
                    return f"{name}, Version: {version}, Date: {date}"
                elif note:
                    return f"{name} ({note})"
            return name
        
        return self.build_complete_output(
            found_items=found_items,
            missing_items=missing_items,
            value="N/A",
            has_pattern_items=False,
            has_waiver_value=False,
            default_file='N/A',
            name_extractor=extract_name,
            found_reason="Status: Success",
            missing_reason="File loading failed",
            found_desc="Netlist/SPEF files loaded successfully",
            missing_desc="Netlist/SPEF loading issues"
        )
```

**_execute_type2**:
```python
def _execute_type2(self, netlist_info: Dict[str, Any], spef_info: Dict[str, Any], 
                       errors: List[str]) -> CheckResult:
        """
        Type 2: Value comparison with automatic waiver.value=0 support
        
        Match required items in pattern_items against netlist/SPEF content.
        Expected value = number of items that should be found (should match pattern_items count).
        
        Waiver Logic (Automatic via build_complete_output):
        - waiver.value = 0: Auto-convert FAIL/WARNâ†’INFO, force PASS [WAIVED_AS_INFO]
                           Auto-parse waive_items and output as INFO [WAIVED_INFO]
        - waiver.value = N/A: Normal mode
        """
        requirements = self.get_requirements()
        pattern_items = requirements.get('pattern_items', []) if requirements else []
        
        found_items = {}
        missing_items = []
        extra_items = {}  # Items not in pattern (SPEF issues, etc.)
        
        # Collect all content to search
        all_content = []
        
        # Add netlist version info
        if netlist_info.get('tool'):
            all_content.append(f"Tool: {netlist_info['tool']}")
        if netlist_info.get('version'):
            all_content.append(f"Genus Synthesis Solution {netlist_info['version']}")
        if netlist_info.get('full_timestamp'):
            all_content.append(f"Generated on: {netlist_info['full_timestamp']}")
        
        # Add SPEF version info
        if spef_info.get('program'):
            all_content.append(f"Program: {spef_info['program']}")
        if spef_info.get('version'):
            all_content.append(f"VERSION {spef_info['version']}")
        if spef_info.get('date'):
            all_content.append(f"DATE {spef_info['date']}")
        
        # Match patterns against content
        matched_patterns = set()
        for pattern in pattern_items:
            found = False
            matched_content = None
            for content in all_content:
                if self._match_pattern(content, [pattern]):
                    found = True
                    matched_content = content
                    break
            
            if found:
                matched_patterns.add(pattern)
                # Build found_items with file/version metadata
                if netlist_info.get('path') and ('Genus' in pattern or 'Generated on' in pattern):
                    metadata = self._metadata.get('netlist_success', {})
                    found_items[pattern] = {
                        'line_number': metadata.get('line_number', ''),
                        'file_path': metadata.get('file_path', ''),
                        'matched': matched_content
                    }
                elif spef_info.get('path') and ('Quantus' in pattern or 'DATE' in pattern or 'VERSION' in pattern):
                    metadata = self._metadata.get('spef_step_end', {})
                    found_items[pattern] = {
                        'line_number': metadata.get('line_number', ''),
                        'file_path': metadata.get('file_path', ''),
                        'matched': matched_content
                    }
                else:
                    found_items[pattern] = {
                        'line_number': '',
                        'file_path': '',
                        'matched': matched_content
                    }
        
        # Find unmatched patterns
        missing_items = [p for p in pattern_items if p not in matched_patterns]
        
        # Check SPEF skip status - add as extra_item if skipped
        if spef_info.get('status') == 'Skipped':
            metadata = self._metadata.get('spef_skipped', {})
            skip_reason = spef_info.get('skip_reason', 'SPEF reading was skipped')
            extra_items["SPEF Reading was skipped"] = {
                'line_number': metadata.get('line_number', ''),
                'file_path': metadata.get('file_path', ''),
                'reason': skip_reason
            }
        
        # Add other errors as extra items
        for error in errors:
            if not any(e in error for e in ["SPEF reading was skipped"]):
                extra_items[f"Error: {error}"] = {
                    'line_number': '',
                    'file_path': '',
                    'reason': 'Unexpected error'
                }
        
        # Name extractor
        def extract_name(name, metadata):
            if isinstance(metadata, dict):
                matched = metadata.get('matched', '')
                reason = metadata.get('reason', '')
                if matched:
                    return f"{name}: {matched}"
                elif reason:
                    return f"{name}: {reason}"
            return name
        
        return self.build_complete_output(
            found_items=found_items,
            missing_items=missing_items,
            extra_items=extra_items,
            value=len(found_items),
            has_pattern_items=True,
            has_waiver_value=False,
            default_file='N/A',
            name_extractor=extract_name,
            found_reason="Version pattern matched",
            missing_reason="Required pattern not found",
            extra_reason="Design has no spef/netlist file or unexpected error",
            found_desc="Netlist/SPEF version is correct",
            missing_desc="Netlist/SPEF version isn't correct",
            extra_desc="Design has no spef/netlist file"
        )
```

**_read_file_content**:
```python
def _read_file_content(self, file_path: Path, max_lines: int = 100) -> List[str]:
        """
        Read file content, supporting both plain text and gzip compressed files.
        
        Args:
            file_path: Path to the file
            max_lines: Maximum number of lines to read from start
            
        Returns:
            List of lines from the file
        """
        if not file_path.exists():
            return []
        
        try:
            # Check if file is actually gzipped by trying to read it
            if file_path.suffix == '.gz':
                try:
                    with gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore') as f:
                        lines = []
                        for i, line in enumerate(f):
                            if i >= max_lines:
                                break
                            lines.append(line)
                        return lines
                except (gzip.BadGzipFile, OSError):
                    # Not a real gzip file, fall through to read as plain text
                    pass
            
            # Read as plain text file
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = []
                for i, line in enumerate(f):
                    if i >= max_lines:
                        break
                    lines.append(line)
                return lines
        except Exception as e:
            print(f"Warning: Failed to read {file_path}: {e}")
            return []
```

**_parse_netlist_version**:
```python
def _parse_netlist_version(self, netlist_path: Path) -> Dict[str, str]:
        """
        Parse netlist file to extract version information.
        
        Expected format:
        // Generated by Cadence Genus(TM) Synthesis Solution 23.15-s099_1
        // Generated on: Nov 18 2025 15:58:15 IST (Nov 18 2025 10:28:15 UTC)
        
        Returns:
            Dict with keys: tool, version, date, time
        """
        version_info = {
            'tool': '',
            'version': '',
            'date': '',
            'time': '',
            'full_timestamp': ''
        }
        
        if not netlist_path.exists():
            return version_info
        
        lines = self._read_file_content(netlist_path, max_lines=50)
        
        for line in lines:
            # Extract tool and version: "Generated by Cadence Genus(TM) Synthesis Solution 23.15-s099_1"
            if 'Generated by' in line and 'Genus' in line:
                # Extract version number
                match = re.search(r'Synthesis Solution\s+([\d\.\-\w]+)', line)
                if match:
                    version_info['tool'] = 'Cadence Genus Synthesis Solution'
                    version_info['version'] = match.group(1)
            
            # Extract generation date: "Generated on: Nov 18 2025 15:58:15 IST"
            elif 'Generated on:' in line:
                # Extract date and time
                match = re.search(r'Generated on:\s+(\w+\s+\d+\s+\d+)\s+([\d:]+)', line)
                if match:
                    version_info['date'] = match.group(1)
                    version_info['time'] = match.group(2)
                    version_info['full_timestamp'] = f"{match.group(1)} {match.group(2)}"
        
        return version_info
```

**_parse_spef_version**:
```python
def _parse_spef_version(self, spef_path: Path) -> Dict[str, str]:
        """
        Parse SPEF file to extract version information.
        
        Expected format:
        *SPEF "IEEE 1481-1999"
        *DESIGN "cdn_sd2101_i3p765_vm130_6x2ya2yb2yc2yd1ye1ga1gb"
        *DATE "Tue Jun 10 14:16:48 2025"
        *VENDOR "Cadence Design Systems Inc"
        *PROGRAM "Cadence Quantus Extraction"
        *VERSION "23.1.0-p075 Tue Sep 26 09:27:40 PDT 2023"
        
        Returns:
            Dict with keys: design, date, vendor, program, version
        """
        version_info = {
            'design': '',
            'date': '',
            'vendor': '',
            'program': '',
            'version': '',
            'spef_standard': ''
        }
        
        if not spef_path.exists():
            return version_info
        
        lines = self._read_file_content(spef_path, max_lines=100)
        
        for line in lines:
            # *SPEF "IEEE 1481-1999"
            if line.startswith('*SPEF'):
                match = re.search(r'\*SPEF\s+"([^"]+)"', line)
                if match:
                    version_info['spef_standard'] = match.group(1)
            
            # *DESIGN "design_name"
            elif line.startswith('*DESIGN'):
                match = re.search(r'\*DESIGN\s+"([^"]+)"', line)
                if match:
                    version_info['design'] = match.group(1)
            
            # *DATE "Tue Jun 10 14:16:48 2025"
            elif line.startswith('*DATE'):
                match = re.search(r'\*DATE\s+"([^"]+)"', line)
                if match:
                    version_info['date'] = match.group(1)
            
            # *VENDOR "Cadence Design Systems Inc"
            elif line.startswith('*VENDOR'):
                match = re.search(r'\*VENDOR\s+"([^"]+)"', line)
                if match:
                    version_info['vendor'] = match.group(1)
            
            # *PROGRAM "Cadence Quantus Extraction"
            elif line.startswith('*PROGRAM'):
                match = re.search(r'\*PROGRAM\s+"([^"]+)"', line)
                if match:
                    version_info['program'] = match.group(1)
            
            # *VERSION "23.1.0-p075 Tue Sep 26 09:27:40 PDT 2023"
            elif line.startswith('*VERSION'):
                match = re.search(r'\*VERSION\s+"([^"]+)"', line)
                if match:
                    version_info['version'] = match.group(1)
        
        return version_info
```

**_resolve_relative_path**:
```python
def _resolve_relative_path(self, relative_path: str, sta_log_dir: Path) -> Optional[Path]:
        """
        Resolve relative path from STA log to absolute path.
        
        Args:
            relative_path: Relative path from STA log (e.g., "../../data/syn_opt/...")
            sta_log_dir: Directory containing the STA log file
            
        Returns:
            Absolute path if exists, None otherwise
        """
        if not relative_path:
            return None
        
        # Try to resolve from STA log directory
        try:
            # Clean path
            clean_path = relative_path.strip().strip('"').strip("'")
            
            # Resolve relative to STA log directory
            abs_path = (sta_log_dir / clean_path).resolve()
            
            if abs_path.exists():
                return abs_path
            
            # If not found, try from project root
            if self.root:
                abs_path = (self.root / clean_path).resolve()
                if abs_path.exists():
                    return abs_path
        except Exception as e:
            print(f"Warning: Failed to resolve path {relative_path}: {e}")
        
        return None
```

**_parse_sta_log**:
```python
def _parse_sta_log(self) -> Dict[str, Any]:
        """
        Parse STA log file to extract netlist/SPEF information.
        
        Returns:
            Dict with keys:
            - netlist_path: Path to netlist file
            - netlist_status: Success/Failed
            - spef_path: Path to SPEF file (if any)
            - spef_status: Success/Failed/Skipped
            - errors: List of error messages
        """
        sta_info = {
            'netlist_path': None,
            'netlist_status': 'Not Found',
            'spef_path': None,
            'spef_status': 'Not Found',
            'errors': [],
            'warnings': []
        }
        
        # Validate input_files configuration
        if not self.item_data or 'input_files' not in self.item_data:
            raise ConfigurationError("No input_files specified in configuration")
        
        input_files = self.item_data['input_files']
        
        # Check if input_files is empty
        if not input_files:
            raise ConfigurationError("input_files is empty in configuration")
        
        if isinstance(input_files, str):
            input_files = [input_files]
        
        # Check if list is empty after conversion
        if not input_files:
            raise ConfigurationError("input_files list is empty")
        
        for file_path_str in input_files:
            file_path = Path(file_path_str)
            
            if not file_path.exists():
                sta_info['errors'].append(f"STA log file not found: {file_path_str}")
                continue
            
            sta_log_dir = file_path.parent
            
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
            
            for line_num, line in enumerate(lines, 1):
                # Extract netlist file path
                # <CMD> read_netlist ../../data/syn_opt/dbs/syn_opt.block_finish.cdb/cmn/phy_cmn_phase_align_digtop.v.gz
                if 'read_netlist' in line and '<CMD>' in line:
                    match = re.search(r'read_netlist\s+(\S+)', line)
                    if match:
                        netlist_rel_path = match.group(1)
                        netlist_abs_path = self._resolve_relative_path(netlist_rel_path, sta_log_dir)
                        # Store path even if file doesn't exist
                        if netlist_abs_path:
                            sta_info['netlist_path'] = netlist_abs_path
                        else:
                            # Store relative path info for reference
                            sta_info['netlist_relative_path'] = netlist_rel_path
                        
                        self._metadata['netlist_cmd'] = {
                            'line_number': line_num,
                            'file_path': str(file_path),
                            'relative_path': netlist_rel_path
                        }
                
                # Reading verilog netlist '...'
                elif 'Reading verilog netlist' in line:
                    match = re.search(r"Reading verilog netlist\s+'([^']+)'", line)
                    if match:
                        netlist_rel_path = match.group(1)
                        if not sta_info.get('netlist_path'):
                            netlist_abs_path = self._resolve_relative_path(netlist_rel_path, sta_log_dir)
                            if netlist_abs_path:
                                sta_info['netlist_path'] = netlist_abs_path
                            else:
                                sta_info['netlist_relative_path'] = netlist_rel_path
                    
                    self._metadata['netlist_reading'] = {
                        'line_number': line_num,
                        'file_path': str(file_path)
                    }
                
                # Check netlist success: "*** Netlist is unique."
                elif '*** Netlist is unique' in line or 'Netlist is unique' in line:
                    sta_info['netlist_status'] = 'Success'
                    self._metadata['netlist_success'] = {
                        'line_number': line_num,
                        'file_path': str(file_path)
                    }
                
                # Check for SPEF reading
                # [INFO] Skipping SPEF reading as we are writing post-synthesis SDF files
                elif 'Skipping SPEF reading' in line:
                    sta_info['spef_status'] = 'Skipped'
                    sta_info['errors'].append("SPEF reading was skipped")
                    self._metadata['spef_skipped'] = {
                        'line_number': line_num,
                        'file_path': str(file_path),
                        'reason': line.strip()
                    }
                
                # read_parasitics step
                elif 'Begin flow_step read_parasitics' in line:
                    self._metadata['spef_step_begin'] = {
                        'line_number': line_num,
                        'file_path': str(file_path)
                    }
                
                elif 'End flow_step read_parasitics' in line:
                    self._metadata['spef_step_end'] = {
                        'line_number': line_num,
                        'file_path': str(file_path)
                    }
                    # If SPEF was not skipped and step completed, assume success
                    if sta_info['spef_status'] != 'Skipped':
                        sta_info['spef_status'] = 'Success'
                
                # Look for SPEF file path
                # read_spef or similar command
                elif 'read_spef' in line.lower() or 'read_parasitics' in line.lower():
                    # Try to extract SPEF file path
                    match = re.search(r'([\w/\.\-]+\.spef(?:\.gz)?)', line, re.IGNORECASE)
                    if match:
                        spef_rel_path = match.group(1)
                        spef_abs_path = self._resolve_relative_path(spef_rel_path, sta_log_dir)
                        if spef_abs_path:
                            sta_info['spef_path'] = spef_abs_path
                
                # Check for errors
                elif re.search(r'\b(error|failed)\b', line, re.IGNORECASE):
                    if 'netlist' in line.lower() or 'spef' in line.lower():
                        sta_info['errors'].append(f"Line {line_num}: {line.strip()}")
        
        # Final status determination
        if sta_info.get('netlist_relative_path') or sta_info.get('netlist_path'):
            # Netlist command found
            if 'netlist_success' in self._metadata:
                sta_info['netlist_status'] = 'Success'
            elif sta_info['netlist_status'] == 'Not Found':
                # Command found but status unknown
                sta_info['netlist_status'] = 'Read Command Found'
        
        return sta_info
```

**_match_pattern**:
```python
def _match_pattern(self, text: str, patterns: List[str]) -> Optional[str]:
        """
        Check if text matches any pattern.
        
        Args:
            text: Text to check
            patterns: List of patterns (supports wildcards)
            
        Returns:
            Matched pattern if found, None otherwise
        """
        for pattern in patterns:
            try:
                # Convert wildcard to regex
                regex_pattern = pattern
                if '*' in pattern and not pattern.startswith('^'):
                    regex_pattern = pattern.replace('*', '.*')
                
                if re.search(regex_pattern, text, re.IGNORECASE):
                    return pattern
            except re.error:
                # Regex error, try exact match
                if pattern.lower() in text.lower():
                    return pattern
        return None
```

**_get_waive_items_with_reasons**:
```python
def _get_waive_items_with_reasons(self) -> Dict[str, str]:
        """
        Get waiver items with their reasons.
        
        Returns:
            Dict mapping waive_item to reason string
        """
        waivers = self.get_waivers()
        if not waivers:
            return {}
        
        waive_items = waivers.get('waive_items', [])
        
        # If it's a list of dicts with 'name' and 'reason'
        if waive_items and isinstance(waive_items[0], dict):
            return {item['name']: item.get('reason', '') for item in waive_items}
        
        # If it's a simple list
        return {item: '' for item in waive_items}
```

## ğŸ“‹ çœŸå® Log æ ·æœ¬ (æ­£åˆ™è¡¨è¾¾å¼è®¾è®¡ä¾æ®)

è¯·åŸºäºä»¥ä¸‹çœŸå®æ—¥å¿—è®¾è®¡æ­£åˆ™è¡¨è¾¾å¼ï¼š

### main
```
Read 20 cells in library 'tcbn03e_bwp143mh286l3p48cpd_mb_svtffgnp_0p825v_m40c_cbest_CCbest_T_ccs'
Reading libset_ffgnp_0p825v_m40c_cbest_CCbest_T_cbest_CCbest_hold_ideal timing library '/process/tsmcN3/data/stdcell/n3ge/TSMC/tcbn03e_bwp143mh286l3p48cpd_mb_svt_110b/lvf/ccs/tcbn03e_bwp143mh286l3p48c
Reading libset_ffgnp_0p825v_m40c_cbest_CCbest_T_cbest_CCbest_hold_ideal timing library '/process/tsmcN3/data/stdcell/n3ge/TSMC/tcbn03e_bwp143mh286l3p48cpd_mb_ulvt_110b/lvf/ccs/tcbn03e_bwp143mh286l3p48
Read 20 cells in library 'tcbn03e_bwp143mh286l3p48cpd_mb_ulvtffgnp_0p825v_m40c_cbest_CCbest_T_ccs'
#@ flow_step:read_parasitics defined in /apps/ssg_tools/p4/nu_global/2025w39.2/common/steps.tcl at line 2758
#@ Begin flow_step read_parasitics
[INFO] Skipping SPEF reading as we are writing post-synthesis SDF files
*** Memory Usage v#1 (Current mem = 2923.969M, initial mem = 828.688M) ***
***** UseNewTieNetMode *****.
[INFO] flowtool/flowtool_flow_status already exists
... (truncated)
```


<semantic_intent>
  <check_target>Verify that the netlist/SPEF version metadata matches expected format or value</check_target>
  <data_flow>STA_Log references SPEF/Netlist files â†’ Extract file paths â†’ Parse SPEF *VERSION or Netlist header â†’ Verify version format/value</data_flow>
  <data_sources>
    <source name="STA_Log" data_role="indirect_reference">
      <role>Contains references to SPEF/netlist files and may contain version information</role>
    </source>
    <source name="SPEF" data_role="direct_source">
      <role>Primary parasitic extraction file containing version metadata</role>
    </source>
    <source name="Netlist_Verilog" data_role="direct_source">
      <role>Synthesized netlist containing tool version in header comments</role>
    </source>
  </data_sources>
  <format_hints>
    <hint format="SPEF">IEEE 1481 standard - version in *VERSION header with quoted string format</hint>
    <hint format="Netlist_Verilog">IEEE 1364/1800 - version typically in header comments with tool name and timestamp</hint>
    <hint format="STA_Log">Tool-specific format - may contain 'Reading SPEF file: <path>' or 'Generated on: <timestamp>' patterns</hint>
  </format_hints>
</semantic_intent>

<context_agent_data>
  <class_constants usage="ç›´æ¥ä½¿ç”¨è¿™äº›å€¼ï¼Œä¸è¦é‡æ–°ç”Ÿæˆ">
    <found_desc>netlist/SPEF version metadata found</found_desc>
    <missing_desc>netlist/SPEF version metadata not found or pattern not satisfied</missing_desc>
    <waived_desc>version check waived for current flow stage</waived_desc>
    <found_reason>Version metadata extracted from {file_type}: {version_string}</found_reason>
    <missing_reason>Version metadata missing in {file_type} or pattern '{pattern}' not matched</missing_reason>
    <waived_base_reason>Version check waived: {waiver_name}</waived_base_reason>
    <extra_reason>Flow stage: {flow_step}, SPEF reading status: {spef_status}</extra_reason>
    <unused_waiver_reason>Waiver '{waiver_name}' defined but not applied (version metadata found)</unused_waiver_reason>
  </class_constants>
  <logic_steps>
    <step order="1">Parse STA log using data_verified patterns (read_netlist, Reading verilog netlist, Top level cell, flow_step, Skipping SPEF) to extract netlist file path, top-level cell name, flow stage, and SPEF skip status</step>
    <step order="2">If SPEF not skipped, parse SPEF file using standard_format patterns (*VERSION, *DESIGN, *DATE, *VENDOR, *PROGRAM per IEEE 1481) to extract version metadata; handle missing SPEF gracefully in synthesis stage</step>
    <step order="3">Parse netlist file header using standard_format patterns (// Generated by, // Date, module name per IEEE 1364/1800) to extract tool version and generation date; use defensive parsing for semantic_inference fields</step>
    <step order="4">For Type 2/3: Search pattern_items (e.g., 'Generated on:*2025*') in extracted version strings using wildcard matching; count matches and compare against requirements.value threshold</step>
    <step order="5">For Type 3/4: Parse waive_items and match against flow stage or SPEF skip status; classify violations as waived (synthesis stage) or unwaived (post-route stage); apply [WAIVER] tag to waived violations in output</step>
  </logic_steps>
  <extraction_chain hint="æŒ‰æ­¤é¡ºåºè§£æå¯è·å¾—æœ€ä¼˜æ•ˆæœ">
    <parse_step order="1" source="chain">STA_Log, extract SPEF/netlist file paths, parse target files for version metadata, extract version string, validate against expected pattern</parse_step>
  </extraction_chain>
  <extraction_fields usage="ç›´æ¥ä½¿ç”¨è¿™äº›æ­£åˆ™æ¨¡å¼">
    <!-- source_type å«ä¹‰: -->
    <!-- data_verified: å·²éªŒè¯ï¼Œç›´æ¥ä½¿ç”¨ -->
    <!-- semantic_inference: æ¨æ–­çš„ï¼Œéœ€è¦ try-except -->
    <!-- standard_format: IEEE/EDA æ ‡å‡†ï¼Œæ·»åŠ æ ¼å¼æ£€æŸ¥ -->
    <file name="STA_Log" data_role="indirect_reference">
      <field name="netlist_file_path" confidence="100%" source="data_verified">
        <pattern><![CDATA[read_netlist\s+(\S+)]]></pattern>
        <purpose>Extracts the netlist file path from the read_netlist command, which is the primary source for netlis</purpose>
      </field>
      <field name="netlist_reading_path" confidence="100%" source="data_verified">
        <pattern><![CDATA[Reading verilog netlist '([^']+)']]></pattern>
        <purpose>Confirms the actual netlist file being read by the tool, provides validation of the file path</purpose>
      </field>
      <field name="top_level_cell" confidence="100%" source="data_verified">
        <pattern><![CDATA[Top level cell is (\w+)\.]]></pattern>
        <purpose>Identifies the top-level design module, useful for cross-referencing with netlist/SPEF metadata</purpose>
      </field>
      <field name="flow_step_name" confidence="100%" source="data_verified">
        <pattern><![CDATA[#@ flow_step:(\w+)]]></pattern>
        <purpose>Captures the current flow step, helps understand the context of the check (e.g., read_parasitics)</purpose>
      </field>
      <field name="spef_skipped_message" confidence="100%" source="data_verified">
        <pattern><![CDATA[\[INFO\]\s*Skipping SPEF reading(.+)]]></pattern>
        <purpose>Detects when SPEF reading is skipped, which affects whether SPEF version can be verified from this l</purpose>
      </field>
      <field name="netlist_unique_status" confidence="100%" source="data_verified">
        <pattern><![CDATA[\*\*\*\s*(Netlist is unique)\.]]></pattern>
        <purpose>Confirms netlist parsing completed successfully, indicates netlist is valid for version extraction</purpose>
      </field>
      <field name="spef_file_path" confidence="95%" source="standard_format">
        <pattern><![CDATA[read_spef\s+(\S+)]]></pattern>
        <purpose>Standard STA command for reading SPEF files, not present in sample but expected in typical STA logs</purpose>
      </field>
      <field name="generated_timestamp" confidence="80%" source="semantic_inference">
        <pattern><![CDATA[Generated on[:\s]+([^\n]+)]]></pattern>
        <purpose>Inferred from semantic intent mentioning 'Generated on:*2025*' pattern, though not present in curren</purpose>
      </field>
    </file>
    <file name="SPEF" data_role="direct_source">
      <field name="spef_version" confidence="95%" source="standard_format">
        <pattern><![CDATA[\*VERSION\s+"([^"]+)"]]></pattern>
        <purpose>IEEE 1481 SPEF standard defines *VERSION header as primary version metadata field</purpose>
      </field>
      <field name="spef_design" confidence="95%" source="standard_format">
        <pattern><![CDATA[\*DESIGN\s+"([^"]+)"]]></pattern>
        <purpose>IEEE 1481 standard - design name for cross-validation with netlist top-level cell</purpose>
      </field>
      <field name="spef_date" confidence="95%" source="standard_format">
        <pattern><![CDATA[\*DATE\s+"([^"]+)"]]></pattern>
        <purpose>IEEE 1481 standard - generation timestamp, can be used to verify year (e.g., 2025)</purpose>
      </field>
      <field name="spef_vendor" confidence="95%" source="standard_format">
        <pattern><![CDATA[\*VENDOR\s+"([^"]+)"]]></pattern>
        <purpose>IEEE 1481 standard - tool vendor information, part of version metadata</purpose>
      </field>
      <field name="spef_program" confidence="95%" source="standard_format">
        <pattern><![CDATA[\*PROGRAM\s+"([^"]+)"]]></pattern>
        <purpose>IEEE 1481 standard - tool program name and version, primary source for tool version verification</purpose>
      </field>
      <field name="spef_standard" confidence="95%" source="standard_format">
        <pattern><![CDATA[\*SPEF\s+"([^"]+)"]]></pattern>
        <purpose>IEEE 1481 standard - SPEF format version (e.g., '1481-2009'), validates file format compliance</purpose>
      </field>
    </file>
    <file name="Netlist_Verilog" data_role="direct_source">
      <field name="netlist_tool_version" confidence="95%" source="standard_format">
        <pattern><![CDATA[//\s*Generated by[:\s]+([^\n]+)]]></pattern>
        <purpose>Standard Verilog netlist header comment containing tool name and version information</purpose>
      </field>
      <field name="netlist_generated_date" confidence="85%" source="semantic_inference">
        <pattern><![CDATA[//\s*Date[:\s]+([^\n]+)]]></pattern>
        <purpose>Common netlist header pattern for generation date, can verify year requirement (e.g., 2025)</purpose>
      </field>
      <field name="netlist_module_name" confidence="95%" source="standard_format">
        <pattern><![CDATA[module\s+(\w+)\s*[#(;]]]></pattern>
        <purpose>IEEE 1364/1800 standard - top module name for cross-validation with STA log top_level_cell</purpose>
      </field>
      <field name="netlist_version_comment" confidence="80%" source="semantic_inference">
        <pattern><![CDATA[//\s*Version[:\s]+([^\n]+)]]></pattern>
        <purpose>Inferred common pattern for version information in netlist header comments</purpose>
      </field>
    </file>
  </extraction_fields>
</context_agent_data>

# ğŸ“‹ ä»»åŠ¡

ç”Ÿæˆ Checker çš„æ ¸å¿ƒæ–¹æ³•:

| å±æ€§ | å€¼ |
|------|-----|
| Item ID | `IMP-10-0-0-00` |
| Class Name | `Check_10_0_0_00` |
| Module | `UNKNOWN` |
| Description | Confirm the netlist/spef version is correct. |

### æ£€æŸ¥ä¸Šä¸‹æ–‡
- **Item**: `IMP-10-0-0-00` (Check_10_0_0_00)
- **æè¿°**: Confirm the netlist/spef version is correct.
- **æ–‡ä»¶ç±»å‹**: Multi-format: STA_Log (primary trigger), SPEF (IEEE 1481 standard), Netlist_Verilog (IEEE 1364/1800 standard)
- **å‚è€ƒæ­£åˆ™**: `read_netlist\s+(\S+)` | `Reading verilog netlist '([^']+)'` | `Top level cell is (\w+)\.` | `#@ flow_step:(\w+)` | `\[INFO\]\s*Skipping SPEF reading(.+)` ... (+13 more)
- **æå–å­—æ®µ**: {'STA_Log': [{'field_name': 'netlist_file_path', 'search_pattern': 'read_netlist\\s+(\\S+)', 'data_type': 'str', 'source_type': 'data_verified', 'confidence': 1.0, 'rationale': 'Extracts the netlist file path from the read_netlist command, which is the primary source for netlist version verification', 'matched_line': '<CMD> read_netlist C:\\Users\\yuyin\\Desktop\\CHECKLIST\\IP_project_folder\\dbs\\phy_cmn_phase_align_digtop.v.gz'}, {'field_name': 'netlist_reading_path', 'search_pattern': "Reading verilog netlist '([^']+)'", 'data_type': 'str', 'source_type': 'data_verified', 'confidence': 1.0, 'rationale': 'Confirms the actual netlist file being read by the tool, provides validation of the file path', 'matched_line': "Reading verilog netlist 'C:\\Users\\yuyin\\Desktop\\CHECKLIST\\IP_project_folder\\dbs\\phy_cmn_phase_align_digtop.v.gz'"}, {'field_name': 'top_level_cell', 'search_pattern': 'Top level cell is (\\w+)\\.', 'data_type': 'str', 'source_type': 'data_verified', 'confidence': 1.0, 'rationale': 'Identifies the top-level design module, useful for cross-referencing with netlist/SPEF metadata', 'matched_line': 'Top level cell is phy_cmn_phase_align_digtop.'}, {'field_name': 'flow_step_name', 'search_pattern': '#@ flow_step:(\\w+)', 'data_type': 'str', 'source_type': 'data_verified', 'confidence': 1.0, 'rationale': 'Captures the current flow step, helps understand the context of the check (e.g., read_parasitics)', 'matched_line': '#@ flow_step:read_parasitics defined in /apps/ssg_tools/p4/nu_global/2025w39.2/common/steps.tcl at line 2758'}, {'field_name': 'spef_skipped_message', 'search_pattern': '\\[INFO\\]\\s*Skipping SPEF reading(.+)', 'data_type': 'str', 'source_type': 'data_verified', 'confidence': 1.0, 'rationale': 'Detects when SPEF reading is skipped, which affects whether SPEF version can be verified from this log', 'matched_line': '[INFO] Skipping SPEF reading as we are writing post-synthesis SDF files'}, {'field_name': 'netlist_unique_status', 'search_pattern': '\\*\\*\\*\\s*(Netlist is unique)\\.', 'data_type': 'str', 'source_type': 'data_verified', 'confidence': 1.0, 'rationale': 'Confirms netlist parsing completed successfully, indicates netlist is valid for version extraction', 'matched_line': '*** Netlist is unique.'}, {'field_name': 'spef_file_path', 'search_pattern': 'read_spef\\s+(\\S+)', 'data_type': 'str', 'source_type': 'standard_format', 'confidence': 0.95, 'rationale': 'Standard STA command for reading SPEF files, not present in sample but expected in typical STA logs', 'matched_line': None}, {'field_name': 'generated_timestamp', 'search_pattern': 'Generated on[:\\s]+([^\\n]+)', 'data_type': 'str', 'source_type': 'semantic_inference', 'confidence': 0.8, 'rationale': "Inferred from semantic intent mentioning 'Generated on:*2025*' pattern, though not present in current sample", 'matched_line': None}], 'SPEF': [{'field_name': 'spef_version', 'search_pattern': '\\*VERSION\\s+"([^"]+)"', 'data_type': 'str', 'source_type': 'standard_format', 'confidence': 0.95, 'rationale': 'IEEE 1481 SPEF standard defines *VERSION header as primary version metadata field', 'matched_line': None}, {'field_name': 'spef_design', 'search_pattern': '\\*DESIGN\\s+"([^"]+)"', 'data_type': 'str', 'source_type': 'standard_format', 'confidence': 0.95, 'rationale': 'IEEE 1481 standard - design name for cross-validation with netlist top-level cell', 'matched_line': None}, {'field_name': 'spef_date', 'search_pattern': '\\*DATE\\s+"([^"]+)"', 'data_type': 'str', 'source_type': 'standard_format', 'confidence': 0.95, 'rationale': 'IEEE 1481 standard - generation timestamp, can be used to verify year (e.g., 2025)', 'matched_line': None}, {'field_name': 'spef_vendor', 'search_pattern': '\\*VENDOR\\s+"([^"]+)"', 'data_type': 'str', 'source_type': 'standard_format', 'confidence': 0.95, 'rationale': 'IEEE 1481 standard - tool vendor information, part of version metadata', 'matched_line': None}, {'field_name': 'spef_program', 'search_pattern': '\\*PROGRAM\\s+"([^"]+)"', 'data_type': 'str', 'source_type': 'standard_format', 'confidence': 0.95, 'rationale': 'IEEE 1481 standard - tool program name and version, primary source for tool version verification', 'matched_line': None}, {'field_name': 'spef_standard', 'search_pattern': '\\*SPEF\\s+"([^"]+)"', 'data_type': 'str', 'source_type': 'standard_format', 'confidence': 0.95, 'rationale': "IEEE 1481 standard - SPEF format version (e.g., '1481-2009'), validates file format compliance", 'matched_line': None}], 'Netlist_Verilog': [{'field_name': 'netlist_tool_version', 'search_pattern': '//\\s*Generated by[:\\s]+([^\\n]+)', 'data_type': 'str', 'source_type': 'standard_format', 'confidence': 0.95, 'rationale': 'Standard Verilog netlist header comment containing tool name and version information', 'matched_line': None}, {'field_name': 'netlist_generated_date', 'search_pattern': '//\\s*Date[:\\s]+([^\\n]+)', 'data_type': 'str', 'source_type': 'semantic_inference', 'confidence': 0.85, 'rationale': 'Common netlist header pattern for generation date, can verify year requirement (e.g., 2025)', 'matched_line': None}, {'field_name': 'netlist_module_name', 'search_pattern': 'module\\s+(\\w+)\\s*[#(;]', 'data_type': 'str', 'source_type': 'standard_format', 'confidence': 0.95, 'rationale': 'IEEE 1364/1800 standard - top module name for cross-validation with STA log top_level_cell', 'matched_line': None}, {'field_name': 'netlist_version_comment', 'search_pattern': '//\\s*Version[:\\s]+([^\\n]+)', 'data_type': 'str', 'source_type': 'semantic_inference', 'confidence': 0.8, 'rationale': 'Inferred common pattern for version information in netlist header comments', 'matched_line': None}]}

<type_specifications hint="è¿è¡Œæ—¶æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©">
  <type id="1" needs_waiver="false">
    <pass_condition>version_found_in_netlist OR version_found_in_spef</pass_condition>
    <fail_condition>NOT version_found_in_netlist AND NOT version_found_in_spef</fail_condition>
  </type>
  <type id="2" needs_waiver="false">
    <pass_condition>all_patterns_matched AND match_count <= requirements.value</pass_condition>
    <fail_condition>NOT all_patterns_matched OR match_count > requirements.value</fail_condition>
  </type>
  <type id="3" needs_waiver="true">
    <pass_condition>all_violations_waived (e.g., synthesis stage waiver covers SPEF pattern mismatch)</pass_condition>
    <fail_condition>unwaived_violations_exist (e.g., post-route SPEF missing 2025 date)</fail_condition>
  </type>
  <type id="4" needs_waiver="true">
    <pass_condition>version_metadata_exists OR flow_stage_matches_waiver</pass_condition>
    <fail_condition>NOT version_metadata_exists AND NOT flow_stage_matches_waiver</fail_condition>
  </type>
</type_specifications>

<runtime_parameters hint="é‡è¦ï¼šå‚æ•°ä» self.item_data è·å–ï¼Œä¸æ˜¯ self.requirements">
  <pattern_items_usage types="Type2,Type3">
    <code_template><![CDATA[
# pattern_items ä» self.item_data è·å– (NOT self.requirements!)
requirements = self.item_data.get('requirements', {})
pattern_items = requirements.get('pattern_items', [])

# éå†åŒ¹é…ç¤ºä¾‹:
for item in pattern_items:
    if isinstance(item, str):
        pattern = item
    else:
        pattern = item.get('pattern', '')
    if pattern in extracted_value:
        matched = True
    ]]></code_template>
  </pattern_items_usage>
  <waive_items_usage types="Type3,Type4">
    <code_template><![CDATA[
# ä½¿ç”¨ WaiverHandlerMixin æä¾›çš„æ–¹æ³•
waivers = self.get_waivers()
waive_items_raw = waivers.get('waive_items', [])
waive_dict = self.parse_waive_items(waive_items_raw)

# åŒ¹é…åˆ¤æ–­ (åœ¨å¾ªç¯ä¸­ä½¿ç”¨):
for violation in violations:
    if self.match_waiver_entry(violation, waive_dict):
        waived_items.append(violation)
    else:
        unwaived_items.append(violation)
    ]]></code_template>
  </waive_items_usage>
</runtime_parameters>

# ğŸ“¤ è¾“å‡ºè¦æ±‚

> **è¯¦ç»† API å¥‘çº¦è§ System Prompt Section 2**

## CRITICAL æé†’

1. **æ–¹æ³•ç­¾å**: `_execute_typeN(self, parsed_data)` - å¿…é¡»æ¥æ”¶ parsed_data
2. **Helper Methods**: `self._xxx()` å¿…é¡»åœ¨ `<helper_methods>` ä¸­å®šä¹‰
3. **Metadata**: è§£ææ—¶ `self._metadata['key'] = {'line_number': N, 'file_path': str}`, ä½¿ç”¨æ—¶ `meta.get('line_number', 0)`
4. **Waiver (Type3/4)**: ä½¿ç”¨ `self.is_item_waived_word_level()` æˆ– word-level åŒ¹é…

## è¾“å‡º XML æ ¼å¼

```xml
<thoughts>
1. æ£€æŸ¥ç›®æ ‡ 2. æ•°æ®æµ 3. è¾…åŠ©æ–¹æ³• 4. Pass/Failé€»è¾‘ 5. Metadataè¿½è¸ª
</thoughts>

<class_constants>
FOUND_DESC = "..." / MISSING_DESC = "..." / ...
</class_constants>

<parse_method>
def _parse_input_files(self) -> Dict[str, Any]: ...
</parse_method>

<execute_type1>...<execute_type2>...<execute_type3>...<execute_type4>

<helper_methods>
# âš ï¸ æ‰€æœ‰ self._xxx() è°ƒç”¨å¿…é¡»åœ¨è¿™é‡Œå®šä¹‰!
</helper_methods>
```